Resumo
A maioria dos estudos sobre atribuição de autoria baseada em estatística ou em aprendizado de máquina se concentra em dois ou alguns autores. Isso leva a uma superestimação da importância dos recursos extraídos dos dados de treinamento e considerados discriminatórios para esses pequenos conjuntos de autores. A maioria dos estudos também usa tamanhos de dados de treinamento que não são realistas para situações nas quais a estilometria é aplicada (por exemplo, forense) e, portanto, superestimam a precisão de sua abordagem nessas situações. Uma interpretação mais realista da tarefa é como um problema de verificação de autoria que aproximamos ao reunir dados de muitos autores diferentes como exemplos negativos. Neste artigo, mostramos, com base em um novo corpus com 145 autores, qual é o efeito de muitos autores na seleção e aprendizado de recursos, e mostramos a robustez de uma abordagem de aprendizado baseado em memória para fazer atribuição e verificação de autoria com muitos autores e dados limitados de treinamento, quando comparados a métodos de aprendizado mais ágeis, como SVMs e aprendizado máximo de entropia.

Introdução
Nos estudos tradicionais sobre atribuição de autoria, o foco está em pequenos conjuntos de autores. Tentar classificar um texto invisível como sendo escrito por um de dois ou de alguns autores é uma tarefa relativamente simples, que na maioria dos casos pode ser resolvida com alta confiabilidade e precisão acima de 95%. Um estudo estatístico inicial de Mosteller e Wallace (1964) adotou distribuições de palavras funcionais como uma característica discriminatória para resolver a autoria contestada dos Federalist Papers entre três autores candidatos (Alexander Hamilton, James Madison e John Jay). A vantagem da distribuição de palavras de função e características sintáticas é que elas não estão sob o controle consciente do autor e, portanto, fornecem boas pistas para a autoria (Holmes, 1994). Frequências de regras de reescrita (Baayen et al., 1996), n-gramas de rótulos sintáticos da análise parcial (Hirst e Feiguina, 2007), n-gramas de partes do discurso (Diederich et al., 2000), palavras de função (Miranda García e Calle Martín, 2007), e características lexicais funcionais (Argamon et al., 2007) têm sido reivindicadas como marcadores de estilo confiáveis. Obviamente, há uma diferença entre declarações sobre tipos de recursos e declarações sobre recursos individuais desse tipo. Por exemplo, pode ser correto afirmar que as distribuições de palavras de função são marcadores importantes da identidade do autor, mas a distribuição de uma palavra de função específica, embora útil para distinguir entre um par específico de autores, pode ser irrelevante ao comparar outro par de autores.
O campo da atribuição de autoria é, contudo, dominado por estudos que superestimam a importância dessas características preditivas específicas em experimentos que discriminam apenas dois ou poucos autores. Levar em consideração um conjunto maior de autores permite calcular o grau de variabilidade encontrado no texto em um único tópico de diferentes (tipos de) recursos. Recentemente, a pesquisa começou a se concentrar na atribuição de autoria a grupos maiores de autores: 8 (Van Halteren, 2005), 20 (Argamon et al., 2003), 114 (Madigan et al., 2005) ou até milhares de autores (Koppel et al., 2006) (consulte a Seção 5).
Um segundo problema nos estudos tradicionais são os tamanhos irreais dos dados de treinamento, o que também torna a tarefa consideravelmente mais fácil. Os pesquisadores tendem a usar mais de 10.000 palavras por autor (Argamon et al., 2007; Burrows, 2007; Gamon, 2004; Hirst e Feiguina, 2007; Madigan et al., 2005; Stamatatos, 2007), que é considerado um mínimo confiável para um conjunto autoral '(Burrows, 2007). Quando não há textos longos disponíveis, por exemplo, em poemas (Coyotl-Morales et al., 2006) ou ensaios de estudantes (Van Halteren, 2005), um grande número de textos curtos é selecionado para treinamento de cada autor. Um dos poucos estudos com foco em textos pequenos é Feiguina e Hirst (2007), mas eles selecionam centenas desses textos curtos (aqui 100, 200 ou 500 palavras). A precisão de qualquer um desses estudos com tamanhos irreais de dados de treinamento é superestimada quando comparada a situações realistas. Quando apenas dados limitados estão disponíveis para um autor específico, a tarefa de atribuição do autor se torna muito mais difícil. No forense, onde geralmente apenas um texto pequeno por autor candidato está disponível, as abordagens tradicionais são menos confiáveis ​​do que o esperado dos resultados relatados.
Neste artigo, apresentamos uma interpretação mais realista da tarefa de atribuição de autoria, viz. como um problema de verificação de autoria. Essa é uma tarefa muito mais natural, pois o grupo de autores em potencial para um documento é essencialmente desconhecido. Os especialistas forenses não apenas querem identificar o autor, dado um pequeno conjunto de suspeitos, mas também querem garantir que o autor não seja outra pessoa que não esteja sob investigação. Eles geralmente lidam com e-mails ou cartas curtos e têm apenas dados limitados disponíveis. A questão central na verificação da autoria é O autor candidato x escreveu o documento? Das três abordagens básicas para verificação de autoria - incluindo também uma abordagem de aprendizado de uma classe (Koppel et al., 2007) -, selecionamos uma abordagem de um contra todos. Essa abordagem é semelhante à investigada por Argamon et al. (2003), que permite uma melhor comparação dos resultados. Com apenas poucas instâncias positivas e um grande número de instâncias negativas para aprender, estamos lidando com distribuições de classe altamente distorcidas.
Mostramos, com base em um novo corpus com 145 autores, qual é o efeito de muitos autores na seleção e aprendizado de recursos, e demonstramos robustez de uma abordagem de aprendizado baseado em memória para realizar a atribuição e verificação de autoria com muitos autores e dados limitados de treinamento quando comparado a métodos de aprendizado mais ágeis, como SVMs e aprendizado máximo de entropia. No que diz respeito à seleção de recursos, descobrimos que tipos semelhantes de recursos tendem a funcionar bem para pequenos e grandes conjuntos de autores, mas que nenhuma generalização pode ser feita sobre recursos individuais. A precisão da classificação é claramente superestimada na atribuição de autoria com poucos autores. Experimentos na verificação de autoria com uma abordagem um contra todos revelam que os métodos de aprendizado de máquina são capazes de classificar corretamente até 56% das instâncias positivas nos dados de teste.
Para nossos experimentos, usamos o Personae corpus, uma coleção de ensaios de estudantes de 145 autores (consulte a Seção 2). A maioria dos estudos em estilometria se concentra no inglês, enquanto o nosso foco é na língua escrita holandesa. No entanto, as técnicas utilizadas são transferíveis para outros idiomas.
Corpus
O corpus de 200.000 palavras Personae1 usado neste estudo consiste em 145 ensaios de 1400 palavras para um documentário sobre Vida Artificial, mantendo assim marcadores de gênero, registro, tópico, idade e nível de escolaridade relativamente constantes. Esses ensaios contêm uma descrição factual do documentário e a opinião dos alunos sobre o assunto. A tarefa era voluntária e os alunos que produziam um ensaio foram recompensados ​​com dois ingressos de cinema. Os alunos também fizeram um teste on-line de Indicador de Tipo Myers-Briggs (MBTI) (Briggs Myers e Myers, 1980) e enviaram seu perfil, o texto e algumas informações do usuário através de um site. Todos os alunos liberaram os direitos autorais do texto e permitiram explicitamente o uso do texto e o perfil de personalidade associado à pesquisa, o que possibilita a distribuição do corpus. O corpus não pode ser usado apenas para experiências de atribuição e verificação de autoria, mas também para previsão de personalidade. Mais informações sobre a motivação por trás do corpus e os resultados de experimentos exploratórios na previsão da personalidade podem ser encontradas em Luyckx & Daelemans (2008).
Metodologia
Abordamos a atribuição e a verificação de autoria como tarefas de categorização automática de texto que rotulam documentos de acordo com um conjunto de categorias predefinidas (Sebastiani, 2002, 3). Como na maioria dos sistemas de categorização de texto, adotamos uma abordagem em duas etapas na qual nosso sistema (i) alcança a seleção automática de recursos que possuem alto valor preditivo para as categorias a serem aprendidas (consulte a Seção 3.1) e (ii) usa aprendizado de máquina algoritmos para aprender a categorizar novos documentos usando os recursos selecionados na primeira etapa (consulte a Seção 3.2).
Extração de recursos
Características sintáticas foram propostas como marcadores de estilo mais confiáveis do que, por exemplo, características no nível de token, uma vez que não estão sob o controle consciente do autor (Baayen et al., 1996; Argamon et al., 2007). Para permitir a seleção de recursos lingüísticos em vez de termos (n-gramas de), são necessárias ferramentas robustas e precisas de análise de texto, como lematizadores, parte de identificadores de voz, chunkers etc. Utilizamos o Analisador Raso com Memória (MBSP) (Daelemans e van den Bosch, 2005), que fornece uma análise incompleta do texto de entrada para extrair recursos sintáticos confiáveis. O MBSP tokeniza a entrada, realiza uma análise parcial, procura por frases substantivas, verbais e outras frases e detecta o sujeito e o objeto da frase e várias outras relações gramaticais.
As palavras ou partes do discurso (n gramas) que ocorrem com mais frequência do que o esperado em qualquer uma das categorias são extraídas automaticamente para cada documento. Usamos a métrica do χ 2 (veja a Figura 1), que calcula a frequência esperada e observada para cada item em cada categoria, para identificar recursos capazes de discriminar as categorias sob investigação.

As distribuições de n-gramas de feições lexicais (lex) são representadas numericamente nos vetores de feições, bem como de n-gramas de partes do discurso de granulação fina (pos) e de granulação grossa (cgp). As palavras de função mais preditivas estão presentes no conjunto de recursos fwd. Para todos esses recursos, o valor χ2 é calculado.
Também é representada uma implementação da métrica Flesch-Kincaid, que indica a legibilidade de um texto, juntamente com seus componentes (viz., Comprimento médio da palavra e da frase) e a proporção do tipo de token (que indica a riqueza do vocabulário) (tok).
